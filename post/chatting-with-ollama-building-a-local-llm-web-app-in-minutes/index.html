<!DOCTYPE html>
<html lang="en-us">
    
    


    <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.49" />

    
    
    

<title>Chatting with Ollama: Building a Local LLM Web App in Minutes • Midnight Programmer</title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Chatting with Ollama: Building a Local LLM Web App in Minutes"/>
<meta name="twitter:description" content="Ollama is a lightweight and user-friendly way to run LLMs locally. No need for complex setups and it makes it super easy to explore AI chat models from the comfort of your own device.
This tutorial is a small part of a broader project I&rsquo;m working on, which involves using local LLMs and vision models to analyze data directly on-device. This approach helps reduce costs and addresses some of the privacy concerns raised by our customers."/>

<meta property="og:title" content="Chatting with Ollama: Building a Local LLM Web App in Minutes" />
<meta property="og:description" content="Ollama is a lightweight and user-friendly way to run LLMs locally. No need for complex setups and it makes it super easy to explore AI chat models from the comfort of your own device.
This tutorial is a small part of a broader project I&rsquo;m working on, which involves using local LLMs and vision models to analyze data directly on-device. This approach helps reduce costs and addresses some of the privacy concerns raised by our customers." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://prashantkhandelwal.github.io/post/chatting-with-ollama-building-a-local-llm-web-app-in-minutes/" /><meta property="article:published_time" content="2025-11-07T05:02:00&#43;00:00"/>
<meta property="article:modified_time" content="2025-11-07T05:02:00&#43;00:00"/><meta property="og:site_name" content="Midnight Programmer" />


    


<link rel="stylesheet" href="/css/hyde-hyde.css">
<link rel="stylesheet" href="/css/print.css" media="print">


    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    <link rel="shortcut icon" href="/favicon.png">
    

</head>


    <body class=" ">
    
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://prashantkhandelwal.github.io/">Midnight Programmer</a>
      </span>
      
      
      
      <div class="author-image">
        <img src="https://prashantkhandelwal.github.io//img/prashantk.jpg" alt="Author Image" class="img--circle img--headshot element--center"> 
      </div>
      
      <p class="site__description">
         Programming For Fun 
      </p>
    </div>
    <div class="collapsible-menu">
      <input type="checkbox" id="menuToggle">
      <label for="menuToggle">Midnight Programmer</label>
      <div class="menu-content">
        <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="/archive/"><i class='fa fa-road'></i>
						<span>Archive</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/top-posts/"><i class='fa fa-star fa-fw'></i>
						<span>Top Posts</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="/contact/"><i class='fa fa-envelope fa-fw'></i>
						<span>Contact</span>
					</a>
				</li>
			 
		
		</li>
	</ul>
</div>

        <section class="social">
	
	
	
	<a href="https://github.com/prashantkhandelwal" rel="me"><i class="fab fa-github fa-lg" aria-hidden="true"></i></a>
	
	
	<a href="http://feeds.feedburner.com/MidnightProgrammer" rel="me"><i class="fa fa-rss fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	<a href="https://instagram.com/prashantkhandelwal" rel="me"><i class="fab fa-instagram fa-lg" aria-hidden="true"></i></a>
	
	
	<a href="https://linkedin.com/in/khandelwalprashant" rel="me"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a>
	
	
	
	<a href="https://medium.com/@prashantkhandelwal" rel="me"><i class="fab fa-medium fa-lg" aria-hidden="true"></i></a>
	
	
	
	
	
	
</section>

      </div>
    </div>
    <p class="copyright">
      &copy; 2025 Pashant Khandelwal
      <a rel="license" href="https://creativecommons.org/licenses/by-nc-sa/2.5/in/"><img style="border-width: 0;" src="https://i.creativecommons.org/l/by-nc-sa/2.5/in/88x31.png" alt="Creative Commons License" class="thinglinkFiltered"></a>
      
    </p>
  </div>
  <div>
  </div>
</div>

        <div class="content container">
            
    <article>
  <header>
    <h1 class="item__title--big">Chatting with Ollama: Building a Local LLM Web App in Minutes</h1>
    
    
<div class="post__meta">
    
    
    <span>
        <i class="fas fa-calendar-alt"></i> Nov 7, 2025
    </span>
    
    
    
    
    
    
    
    
    <a class="badge badge-category" href="/category/ai">AI</a>
    &nbsp;
    
    <a class="badge badge-category" href="/category/ollama">OLLAMA</a>
    &nbsp;
    
    <a class="badge badge-category" href="/category/svelte">SVELTE</a>
    &nbsp;
    
    <a class="badge badge-category" href="/category/web">WEB</a>
    
    
    
    
    
    
    <br />
    
</div>

  </header>
  
  
  <div class="post">
    <p><a href="https://ollama.com/">Ollama</a> is a lightweight and user-friendly way to run LLMs locally. No need for complex setups and it makes it super easy to explore AI chat models from the comfort of your own device.</p>

<p>This tutorial is a small part of a broader project I&rsquo;m working on, which involves using local LLMs and vision models to analyze data directly on-device. This approach helps reduce costs and addresses some of the privacy concerns raised by our customers.</p>

<p><strong>Installation and Setup</strong></p>

<p>Download and install Ollama from <a href="https://ollama.com/download">here</a>.</p>

<p>Once the setup is complete, simply launch the Ollama application—it will open a ChatGPT-like interface that lets you interact with local LLMs.</p>

<p><img src="/images/ollama-app.png" alt="Ollama App UI" /></p>

<p>This UI makes it very easy for searching, downloading and communicating with different LLMs. You can also chat with the models which are in the cloud without downloading them. Note that you require a Ollama account in order to communicate with a cloud model.</p>

<p>But we need to build a web based chat appliaction and that means that we have to interact with Ollama API which is running at <code>https://localhost:11434</code></p>

<p><img src="/images/ollama-api-running.png" alt="Ollama API" /></p>

<p>Everyting seems to be set up properly. Let&rsquo;s create a Python FastAPI endpoint which allows us to communicate with Ollama API. You can also use NodeJS, Go or .NET WebAPI to create a service endpoint.</p>

<p>Create a Python virtual environment and install the below dependencies.</p>

<pre class="brush:shell">
pip install fastapi uvicorn requests httpx
</pre>

<p>The API uses a POST request and accepts three parameters: <code>prompt</code>, <code>model</code>, and <code>stream</code>.</p>

<ul>
<li><code>prompt</code> – The input message or query from the user.</li>
<li><code>model</code> – Specifies which model to run the prompt against. If not provided, it defaults to l<code>lama3.2:latest</code>.</li>
<li><code>stream</code> – Optional setting that defaults to <code>false</code>. Set it to <code>true</code> if you want the response to appear in a typing animation, similar to ChatGPT. Note: enabling streaming requires additional changes to the code below.</li>
</ul>

<blockquote>
<p>For below version of code, requests and httpx packages are not required.</p>
</blockquote>

<pre class="brush:python">
from fastapi import FastAPI
from pydantic import BaseModel
import requests

app = FastAPI()

class PromptRequest(BaseModel):
    prompt: str
    model: str = "llama3.2:latest"  # Default model, can be overridden in the request

@app.post("/generate")
async def generate_text(request: PromptRequest):
    ollama_api_url = "http://localhost:11434/api/generate"
    
    payload = {
        "model": request.model,
        "prompt": request.prompt,
        "stream": False # True for streaming responses
    }
    
    try:
        response = requests.post(ollama_api_url, json=payload)
        response.raise_for_status()  # Raise an exception for bad status codes
        
        # Extract the generated text from Ollama's response
        generated_text = response.json()["response"]
        return {"response": generated_text}
        
    except requests.exceptions.RequestException as e:
        return {"error": f"Error communicating with Ollama: {e}"}
</pre>

<p>Run this API using <code>uvicorn</code>.</p>

<pre class="brush:shell">
uvicorn main:app
</pre>

<p>The API server will start on default <code>8000</code> port. If you wish to change the port then start the API using the below command.</p>

<pre class="brush:shell">
uvicorn main:app --port 8080
</pre>

<p>Let&rsquo;s check the API reponse using Postman.</p>

<p><img src="/images/ollama-fastapi-postman.png" alt="Postman API Call" /></p>

<p>It’s quite helpful to see the response streamed in real time, just like how ChatGPT displays it. So let&rsquo;s change the <code>stream</code> parameter to <code>true</code> and update our API code.</p>

<pre class="brush:python highlight: [21]">
from fastapi import FastAPI
from fastapi.responses import StreamingResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import httpx
import json
import os

app = FastAPI()

class PromptRequest(BaseModel):
    prompt: str
    model: str = "llama3.2:latest"

@app.post("/generate")
async def generate_text(request: PromptRequest):
    ollama_api_url = "http://localhost:11434/api/generate"
    payload = {
        "model": request.model,
        "prompt": request.prompt,
        "stream": True
    }

    async def stream_text():
        async with httpx.AsyncClient(timeout=None) as client:
            async with client.stream("POST", ollama_api_url, json=payload) as response:
                async for line in response.aiter_lines():
                    if line.strip():
                        try:
                            data = json.loads(line)
                            chunk = data.get("response", "")
                            if chunk:
                                yield chunk
                        except json.JSONDecodeError:
                            continue

    return StreamingResponse(stream_text(), media_type="text/plain")

</pre>

<p>Now we have a streaming response, let&rsquo;s make a UI, I am using Svelte. Start by creating a new project.</p>

<pre class="brush:shell">
npm create vite@latest ollama-chat -- --template svelte-ts
</pre>

<p>Update the <code>vite.config.ts</code> file to include a custom proxy setting for the development server. This setup ensures that any requests made to <code>/generate</code> are forwarded to <code>http://localhost:8000</code>, allowing the frontend to communicate seamlessly with a backend API like FastAPI. It also helps prevent CORS-related issues during development.</p>

<pre class="brush:js">
export default defineConfig({
  plugins: [svelte()],
   server: {
    proxy: {
      '/generate': 'http://localhost:8000'
    }
  }
})
</pre>

<p>The response is formatted in <code>Markdown</code>, so to render it correctly, you&rsquo;ll need an additional npm package called <code>marked</code>. You can install it using the command below.</p>

<pre class="brush:shell">
npm install marked
</pre>

<blockquote>
<p>Remember to change the port if your have setup the custom port for your API via uvicorn.</p>
</blockquote>

<p>Replace the code in <code>App.svelte</code> with the below code.</p>

<pre class="brush:jscript">
<script lang="ts">
  import { marked } from 'marked';

  let prompt: string = '';
  let chat: string = '';
  let chatHtml: string = '';
  let loading: boolean = false;

  async function sendPrompt(): Promise<void> {
    if (!prompt.trim()) return;
    chat = '';
    chatHtml = '';
    loading = true;

    const res = await fetch('/generate', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ prompt, model: 'llama3.2:latest' })
    });

    const reader = res.body?.getReader();
    const decoder = new TextDecoder('utf-8');

    if (reader) {
      while (true) {
        const { value, done } = await reader.read();
        if (done) break;
        const chunk = decoder.decode(value, { stream: true });
        chat += chunk;
        chatHtml = marked(chat); // Convert Markdown to HTML
      }
    }

    loading = false;
  }
</script>

<style>
  textarea {
    width: 100%;
    height: 100px;
    margin-bottom: 10px;
  }
  button {
    padding: 10px 20px;
  }
  #chat {
    margin-top: 20px;
    background: #243434;
    padding: 10px;
    border-radius: 5px;
    white-space: pre-wrap;
  }
  .chat-output {
    text-align: left;
  }
</style>

<h2>Ollama Chat</h2>
<label for="prompt">Enter your prompt:</label><br>
<textarea bind:value={prompt} id="prompt"></textarea><br>
<button on:click={sendPrompt} disabled={loading}>
  {loading ? 'Sending...' : 'Send'}
</button>

<div id="chat">
  {#if loading}
    <p>Loading...</p>
  {:else if chatHtml}
    <div class="chat-output">
      {@html chatHtml}
    </div>
  {:else}
    <p>No response yet.</p>
  {/if}
</div>
</style>

<h2>Ollama Chat</h2>
<label for="prompt">Enter your prompt:</label><br>
<textarea bind:value={prompt} id="prompt"></textarea><br>
<button on:click={sendPrompt} disabled={loading}>
  {loading ? 'Sending...' : 'Send'}
</button>

<div id="chat">{chat}</div>
</pre>

<p>Start the UI using the command</p>

<pre class="brush:shell">
npm run dev
</pre>

<p>We are now all set to run our local LLM based chat agent. Let&rsquo;s start by asking a question.</p>

<p><img src="/images/ollama-chat-output.png" alt="Ollama web chat interface" /></p>

<p>This code serves as a starting point. You can extend it by adding image or file upload functionality, allowing users to summarize content or ask questions based on the data within the uploaded document or image.</p>

<p>Here is the <a href="https://github.com/prashantkhandelwal/ollama-chat">Github repo</a> where you can find the entire code.</p>

  </div>
  <div style="text-align:center; margin-top:10px">
    <script>
      (adsbygoogle = window.adsbygoogle || []).push({
        google_ad_client: "ca-pub-5924643983709476",
        enable_page_level_ads: true
      });
    </script>
  </div>
  

<div class="navigation navigation-single">
    
    <a href="/post/simple-and-cheap-create-a-diy-mouse-jiggler-with-raspberry-pi-pico/" class="navigation-prev">
      <i aria-hidden="true" class="fa fa-chevron-left"></i>
      <span class="navigation-tittle">Simple and Cheap: Create a DIY Mouse Jiggler with Raspberry Pi Pico</span>
    </a>
    
    
</div>


  

  
    
        

<div id="disqus_thread"></div>
<script type="text/javascript">
    

    (function () {
    if (location.hostname === "localhost" ||
      location.hostname === "127.0.0.1" ||
      location.hostname === "") {
      return;
    }
    var dsq = document.createElement('script');
    dsq.type = 'text/javascript';
    dsq.async = true;
    var disqus_shortname = 'localblog-3';
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || 
      document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>

<noscript>
  Please enable JavaScript to view the
  <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>
<a href="http://disqus.com/" class="dsq-brlink">comments powered by
  <span class="logo-disqus">Disqus</span>
</a>

    


</article>
 




        </div>
        
    
  
  
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'G-QYXJT2TJCG', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script defer src="https://use.fontawesome.com/releases/v5.2.0/js/all.js"
  integrity="sha384-4oV5EgaV02iISL2ban6c/RmotsABqE4yZxZLcYMAdG7FAPsyHYAPpywE9PJo+Khy"
  crossorigin="anonymous">
</script>

<link
  href="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/styles/shCore.min.css"
  rel="stylesheet"
/>
<link
  href="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/styles/shThemeDefault.min.css"
  rel="stylesheet"
/>

<script
  type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shCore.min.js"
></script>
<script
  type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shBrushCSharp.min.js"
></script>
<script
  type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shBrushCss.min.js"
></script>
<script
  type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shBrushJScript.min.js"
></script>
<script
  type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shBrushPlain.min.js"
></script>
<script
  type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shBrushSql.min.js"
></script>
<script
  type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shBrushXml.min.js"
></script>
<script
  type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shBrushPowerShell.min.js"
></script>
<script
  type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shBrushPython.min.js"
></script>
<script
  type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shBrushBash.min.js"
></script>
<script
  type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shBrushCpp.min.js"
></script>
<script
  type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/SyntaxHighlighter/3.0.83/scripts/shBrushRuby.min.js"
></script>

<script>
  SyntaxHighlighter.defaults["gutter"] = true;
  SyntaxHighlighter.defaults["smart-tabs"] = true;
  SyntaxHighlighter.defaults["auto-links"] = true;
  SyntaxHighlighter.defaults["collapse"] = false;
  SyntaxHighlighter.defaults["light"] = false;
  SyntaxHighlighter.defaults["tab-size"] = 4;
  SyntaxHighlighter.defaults["toolbar"] = false;
  SyntaxHighlighter.defaults["wrap-lines"] = true;
  SyntaxHighlighter.all();
</script>



    



    </body>
</html>
